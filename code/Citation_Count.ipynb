{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4a204a",
   "metadata": {},
   "source": [
    "# Citatation Counting\n",
    "\n",
    "This notebook counts references to canonical authors in articles by ngrams. This guide will help users update works and get new word counts for new authors by outlining and explaining key code sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d798d7a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b275bc",
   "metadata": {},
   "source": [
    "First, we will ensure we have all the necessary tools installed and imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "206819ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (4.62.2)\n",
      "Requirement already satisfied: tables in /opt/conda/lib/python3.9/site-packages (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.9.3 in /opt/conda/lib/python3.9/site-packages (from tables) (1.20.3)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in /opt/conda/lib/python3.9/site-packages (from tables) (2.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install tables\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86d9b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed385f",
   "metadata": {},
   "source": [
    "Next, we will set up the necessary filepaths to read our JSTOR data. First, we will define our JSTOR HOME data filepath. Next, we'll define our ngram.txt filepaths by reading in filtered_index.csv. Finally, we will read in the expanded dictionary files for cultural, relational, and demographic sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e512f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSTOR_HOME = \"../../jstor_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fee9d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDICES = \"./filtered_index.csv\"\n",
    "\n",
    "with open(INDICES, 'r') as f:\n",
    "    files = f.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e8f44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanded_dict_folder = \"../Dictionaries/Expanded/wordnet_english2/\"\n",
    "expanded_dict_folder = \"../dictionaries/expanded/\"\n",
    "full_cultural = expanded_dict_folder + \"closest_culture_1000.csv\"\n",
    "full_relational = expanded_dict_folder + \"closest_relational_1000.csv\"\n",
    "full_demographic = expanded_dict_folder + \"closest_demographic_1000.csv\"\n",
    "full_cultural_set = set()\n",
    "full_relational_set = set()\n",
    "full_demographic_set = set()\n",
    "\n",
    "csv_lst = [full_cultural, full_demographic, full_relational]\n",
    "set_lst = [full_cultural_set, full_demographic_set, full_relational_set]\n",
    "\n",
    "for i in range(3):\n",
    "    with open(csv_lst[i], 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            set_lst[i].add(' '.join(line[0].split('_')))\n",
    "\n",
    "        \n",
    "all_terms = set.union(full_cultural_set, full_relational_set, full_demographic_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ffc84",
   "metadata": {},
   "source": [
    "## Update Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702a637",
   "metadata": {},
   "source": [
    "Here, we'll create a list of the authors we want to find within our citation, categorizing them as demographic, relational, or cultural. Edit the python lists in the following code snipped by adding or removing authors in order to get new or updated word counts for specific authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0860fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_authors = ['hannan freeman', 'barnett carroll', 'barron west', 'brüderl schüssler', 'carrol hannan', \n",
    "                       'freeman carrol', 'fichman levinthal', 'carrol']\n",
    "## Will leaving in single authors catch extra citations?\n",
    "\n",
    "relational_authors = ['pfeffer salancik', 'burt christman', 'pfeffer nowak', 'pfeffer']\n",
    "\n",
    "cultural_authors = ['meyer rowan', 'dimaggio powell', 'powell dimaggio', 'oliver', 'powell', 'scott', 'weick']\n",
    "\n",
    "ALL_AUTHORS = set(demographic_authors + relational_authors + cultural_authors)\n",
    "\n",
    "author_types_list = [cultural_authors, demographic_authors, relational_authors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba0509",
   "metadata": {},
   "source": [
    "## Word Counts\n",
    "\n",
    "Using the ngram.txt files we previously read in from filtered_index.csv, we will now parse the ngram files and collect and store the word counts of various authors mentioned in the citations of the JSTOR Articles. After creating this dataframe, we can take a look at the total number of words for each category (i.e. cultural, demographic, relational) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d5088",
   "metadata": {},
   "source": [
    "First, we'll create our dataframe that will contain our final information, as well as a list containing the names of the various perspectives we use when collecting ngram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c3eb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = pd.DataFrame(columns=[\"article_id\", \"cultural_author_count\", \"demographic_author_count\", \"relational_author_count\",\n",
    "                                 \"cultural_count2\", \"relational_count2\", \"demographic_count2\",\n",
    "                                 \"cultural_count1\", \"relational_count1\", \"demographic_count1\"]) \n",
    "\n",
    "perspective_types = [\"cultural\", \"demographic\", \"relational\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef43c81",
   "metadata": {},
   "source": [
    "The following method will generate ngram counts by parsing through our JSTOR article files, and collecting and storing the word counts for the authors mentioned in the JSTOR article. This method takes in a **NGRAM value** (1 = unigram, 2 = bigram, etc.) as well as the **dataframe to update** (in this case, the `counts_df` dataframe we created previously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06fc791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_counts(ngram_value, counts_df):\n",
    "    folder = os.path.join(JSTOR_HOME, 'ngram{}'.format(ngram_value))\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        with open(os.path.join(folder, '{}-ngram{}.txt'.format(file, ngram_value)), 'r') as f:\n",
    "\n",
    "            d = {}\n",
    "\n",
    "            for line in f.read().splitlines():\n",
    "                k, v = line.split('\\t')\n",
    "                if k in ALL_AUTHORS or k in all_terms:\n",
    "                    d[k] = int(v)\n",
    "                    \n",
    "            author_sums = [sum([d.get(author, 0) for author in author_list]) for author_list in author_types_list]\n",
    "            term_sums = [sum([d.get(term, 0) for term in set_type]) for set_type in set_lst]\n",
    "                    \n",
    "            if (ngram_value == 1):\n",
    "                for i in range(3):\n",
    "                    counts_df.at[file, perspective_types[i] + \"_author_count\"] = author_sums[i]\n",
    "                    counts_df.at[file, perspective_types[i] + \"_count1\"] = term_sums[i]\n",
    "                \n",
    "\n",
    "            elif (ngram_value == 2):\n",
    "                row = {\"article_id\": file}\n",
    "                for i in range(3):\n",
    "                    row[perspective_types[i] + \"_author_count\"] = author_sums[i]\n",
    "                    row[perspective_types[i] + \"_count2\"] = term_sums[i]\n",
    "\n",
    "                counts_df = counts_df.append(row, ignore_index=True)\n",
    "            \n",
    "            counts_df = counts_df.set_index('article_id')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b930f",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c13868",
   "metadata": {},
   "source": [
    "For this section, we will specifically look at **ngram = 2 words (i.e. bigrams)**. After creating this dataframe, we can take a look at the total number of words for each category (i.e. cultural, demographic, relational) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb3885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7047ce4d1a7d4f909038871eb9fea00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_ngram_counts(2, counts_df)\n",
    "counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e868b",
   "metadata": {},
   "source": [
    "### Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc4e324",
   "metadata": {},
   "source": [
    "We will use the same method as used in the previous section, and will specifically look at **ngram = 1 words (i.e. unigrams)**. After creating this dataframe, we can take a look at the total number of words for each category (i.e. cultural, demographic, relational)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1195a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ngram_counts(1, counts_df)\n",
    "counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59178eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25737edd",
   "metadata": {},
   "source": [
    "## Wrap-up + storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1451f62",
   "metadata": {},
   "source": [
    "We can now write our dataframe back to our csv files to later parse and use. Uncomment the below line to write back to the `citation_and_expanded_dict_count_may7.csv` file, or change the passed in file name to write to a different file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba99ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_df.to_csv('citation_and_expanded_dict_count_may7.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
